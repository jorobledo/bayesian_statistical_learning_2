{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a8ed8ae",
   "metadata": {},
   "source": [
    "# Notation\n",
    "$\\newcommand{\\ve}[1]{\\mathit{\\boldsymbol{#1}}}$\n",
    "$\\newcommand{\\ma}[1]{\\mathbf{#1}}$\n",
    "$\\newcommand{\\pred}[1]{\\rm{#1}}$\n",
    "$\\newcommand{\\predve}[1]{\\mathbf{#1}}$\n",
    "$\\newcommand{\\cov}{\\mathrm{cov}}$\n",
    "$\\newcommand{\\test}[1]{#1_*}$\n",
    "$\\newcommand{\\testtest}[1]{#1_{**}}$\n",
    "\n",
    "Vector $\\ve a\\in\\mathbb R^n$ or $\\mathbb R^{n\\times 1}$, so \"column\" vector.\n",
    "Matrix $\\ma A\\in\\mathbb R^{n\\times m}$. Design matrix with input vectors $\\ve\n",
    "x_i\\in\\mathbb R^D$: $\\ma X = [\\ldots, \\ve x_i, \\ldots]^\\top \\in\\mathbb\n",
    "R^{N\\times D}$.\n",
    "\n",
    "We use 1D data, so in fact $\\ma X \\in\\mathbb R^{N\\times 1}$ is a vector, but\n",
    "we still denote the collection of all $\\ve x_i = x_i\\in\\mathbb R$ points with\n",
    "$\\ma X$ to keep the notation consistent with the slides."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae440984",
   "metadata": {},
   "source": [
    "# Imports, helpers, setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b39064",
   "metadata": {},
   "outputs": [],
   "source": [
    "##%matplotlib notebook\n",
    "%matplotlib widget\n",
    "##%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960de85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import is_interactive\n",
    "\n",
    "from utils import extract_model_params, plot_samples\n",
    "\n",
    "\n",
    "# Default float32 results in slightly noisy prior samples. Less so with\n",
    "# float64. We get a warning with both\n",
    "#   .../lib/python3.11/site-packages/linear_operator/utils/cholesky.py:40:\n",
    "#       NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal\n",
    "# but the noise is smaller w/ float64. Reason must be that the `sample()`\n",
    "# method [1] calls `rsample()` [2] which performs a Cholesky decomposition of\n",
    "# the covariance matrix. The default in\n",
    "# np.random.default_rng().multivariate_normal() is method=\"svd\", which is\n",
    "# slower but seemingly a bit more stable.\n",
    "#\n",
    "# [1] https://docs.gpytorch.ai/en/stable/distributions.html#gpytorch.distributions.MultivariateNormal.sample\n",
    "# [2] https://docs.gpytorch.ai/en/stable/distributions.html#gpytorch.distributions.MultivariateNormal.rsample\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a41bb4f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Generate toy 1D data\n",
    "\n",
    "Here we generate noisy 1D data `X_train`, `y_train` as well as an extended\n",
    "x-axis `X_pred` which we use later for prediction also outside of the data\n",
    "range (extrapolation). The data has a constant offset `const` which we use to\n",
    "test learning a GP mean function $m(\\ve x)$. We create a gap in the data to\n",
    "show how the model uncertainty will behave there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30019345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(x, gaps=[[1, 3]], const=5):\n",
    "    y = torch.sin(x) * torch.exp(-0.2 * x) + torch.randn(x.shape) * 0.1 + const\n",
    "    msk = torch.tensor([True] * len(x))\n",
    "    if gaps is not None:\n",
    "        for g in gaps:\n",
    "            msk = msk & ~((x > g[0]) & (x < g[1]))\n",
    "    return x[msk], y[msk]\n",
    "\n",
    "\n",
    "x = torch.linspace(0, 4 * math.pi, 100)\n",
    "X_train, y_train = generate_data(x, gaps=[[6, 10]])\n",
    "X_pred = torch.linspace(\n",
    "    X_train[0] - 2, X_train[-1] + 2, 200, requires_grad=False\n",
    ")\n",
    "\n",
    "print(f\"{X_train.shape=}\")\n",
    "print(f\"{y_train.shape=}\")\n",
    "print(f\"{X_pred.shape=}\")\n",
    "\n",
    "plt.scatter(X_train, y_train, marker=\"o\", color=\"tab:blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d326cde",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Define GP model\n",
    "\n",
    "We define the simplest possible textbook GP model using a Gaussian\n",
    "likelihood. The kernel is the squared exponential kernel with a scaling\n",
    "factor.\n",
    "\n",
    "$$\\kappa(\\ve x_i, \\ve x_j) = s\\,\\exp\\left(-\\frac{\\lVert\\ve x_i - \\ve x_j\\rVert_2^2}{2\\,\\ell^2}\\right)$$\n",
    "\n",
    "This makes two hyper params, namely the length scale $\\ell$ and the scaling\n",
    "$s$. The latter is implemented by wrapping the `RBFKernel` with\n",
    "`ScaleKernel`.\n",
    "\n",
    "In addition, we define a constant mean via `ConstantMean`. Finally we have\n",
    "the likelihood noise $\\sigma_n^2$. So in total we have 4 hyper params.\n",
    "\n",
    "* $\\ell$ = `model.covar_module.base_kernel.lengthscale`\n",
    "* $\\sigma_n^2$ = `model.likelihood.noise_covar.noise`\n",
    "* $s$ = `model.covar_module.outputscale`\n",
    "* $m(\\ve x) = c$ = `model.mean_module.constant`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce2844b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    \"\"\"API:\n",
    "\n",
    "    model.forward()             prior                   f_pred\n",
    "    model()                     posterior               f_pred\n",
    "\n",
    "    likelihood(model.forward()) prior with noise        y_pred\n",
    "    likelihood(model())         posterior with noise    y_pred\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X_train, y_train, likelihood):\n",
    "        super().__init__(X_train, y_train, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"The prior, defined in terms of the mean and covariance function.\"\"\"\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(X_train, y_train, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a2aab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3989c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default start hyper params\n",
    "pprint(extract_model_params(model, raw=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3808b3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Set new start hyper params\n",
    "model.mean_module.constant = 3.0\n",
    "model.covar_module.base_kernel.lengthscale = 1.0\n",
    "model.covar_module.outputscale = 1.0\n",
    "model.likelihood.noise_covar.noise = 0.1\n",
    "\n",
    "pprint(extract_model_params(model, raw=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3600af3",
   "metadata": {},
   "source": [
    "# Sample from the GP prior\n",
    "\n",
    "We sample a number of functions $f_j, j=1,\\ldots,M$ from the GP prior and\n",
    "evaluate them at all $\\ma X$ = `X_pred` points, of which we have $N=200$. So\n",
    "we effectively generate samples from $p(\\predve f|\\ma X) = \\mathcal N(\\ve\n",
    "c, \\ma K)$. Each sampled vector $\\predve f\\in\\mathbb R^{N}$ and the\n",
    "covariance (kernel) matrix is $\\ma K\\in\\mathbb R^{N\\times N}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74b8d1b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Prior\n",
    "    M = 10\n",
    "    pri_f = model.forward(X_pred)\n",
    "    f_mean = pri_f.mean\n",
    "    f_std = pri_f.stddev\n",
    "    f_samples = pri_f.sample(sample_shape=torch.Size((M,)))\n",
    "    print(f\"{pri_f=}\")\n",
    "    print(f\"{pri_f.mean.shape=}\")\n",
    "    print(f\"{pri_f.covariance_matrix.shape=}\")\n",
    "    print(f\"{f_samples.shape=}\")\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(X_pred, f_mean, color=\"tab:red\", label=\"mean\", lw=2)\n",
    "    plot_samples(ax, X_pred, f_samples, label=\"prior samples\")\n",
    "    ax.fill_between(\n",
    "        X_pred,\n",
    "        f_mean - 2 * f_std,\n",
    "        f_mean + 2 * f_std,\n",
    "        color=\"tab:orange\",\n",
    "        alpha=0.2,\n",
    "        label=\"confidence\",\n",
    "    )\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0598b567",
   "metadata": {},
   "source": [
    "Let's investigate the samples more closely. A constant mean $\\ve m(\\ma X) =\n",
    "\\ve c$ does *not* mean that each sampled vector $\\predve f$'s mean is\n",
    "equal to $c$. Instead, we have that at each $\\ve x_i$, the mean of\n",
    "*all* sampled functions is the same, so $\\frac{1}{M}\\sum_{j=1}^M f_j(\\ve x_i)\n",
    "\\approx c$ and for $M\\rightarrow\\infty$ it will be exactly $c$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7fa03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the first 20 x points from M=10 samples\n",
    "print(f\"{f_samples.shape=}\")\n",
    "print(f\"{f_samples.mean(axis=0)[:20]=}\")\n",
    "print(f\"{f_samples.mean(axis=0).mean()=}\")\n",
    "print(f\"{f_samples.mean(axis=0).std()=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b6e415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take more samples, the means should get closer to c\n",
    "f_samples = pri_f.sample(sample_shape=torch.Size((M * 200,)))\n",
    "print(f\"{f_samples.shape=}\")\n",
    "print(f\"{f_samples.mean(axis=0)[:20]=}\")\n",
    "print(f\"{f_samples.mean(axis=0).mean()=}\")\n",
    "print(f\"{f_samples.mean(axis=0).std()=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b90532",
   "metadata": {},
   "source": [
    "# GP posterior predictive distribution with fixed hyper params\n",
    "\n",
    "Now we calculate the posterior predictive distribution $p(\\test{\\predve\n",
    "f}|\\test{\\ma X}, \\ma X, \\ve y)$, i.e. we condition on the train data (Bayesian\n",
    "inference).\n",
    "\n",
    "We use the fixed hyper param values defined above. In particular, since\n",
    "$\\sigma_n^2$ = `model.likelihood.noise_covar.noise` is > 0, we have a\n",
    "regression setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01951f1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Evaluation (predictive posterior) mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    M = 10\n",
    "    post_pred_f = model(X_pred)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    f_mean = post_pred_f.mean\n",
    "    f_samples = post_pred_f.sample(sample_shape=torch.Size((M,)))\n",
    "    f_std = post_pred_f.stddev\n",
    "    lower = f_mean - 2 * f_std\n",
    "    upper = f_mean + 2 * f_std\n",
    "    ax.plot(\n",
    "        X_train.numpy(),\n",
    "        y_train.numpy(),\n",
    "        \"o\",\n",
    "        label=\"data\",\n",
    "        color=\"tab:blue\",\n",
    "    )\n",
    "    ax.plot(\n",
    "        X_pred.numpy(),\n",
    "        f_mean.numpy(),\n",
    "        label=\"mean\",\n",
    "        color=\"tab:red\",\n",
    "        lw=2,\n",
    "    )\n",
    "    ax.fill_between(\n",
    "        X_pred.numpy(),\n",
    "        lower.numpy(),\n",
    "        upper.numpy(),\n",
    "        label=\"confidence\",\n",
    "        color=\"tab:orange\",\n",
    "        alpha=0.3,\n",
    "    )\n",
    "    y_min = y_train.min()\n",
    "    y_max = y_train.max()\n",
    "    y_span = y_max - y_min\n",
    "    ax.set_ylim([y_min - 0.3 * y_span, y_max + 0.3 * y_span])\n",
    "    plot_samples(ax, X_pred, f_samples, label=\"posterior pred. samples\")\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b960a745",
   "metadata": {},
   "source": [
    "# Fit GP to data: optimize hyper params\n",
    "\n",
    "In each step of the optimizer, we condition on the training data (e.g. do\n",
    "Bayesian inference) to calculate the posterior predictive distribution for\n",
    "the current values of the hyper params. We iterate until the log marginal\n",
    "likelihood is converged.\n",
    "\n",
    "We use a simplistic PyTorch-style hand written train loop without convergence\n",
    "control, so make sure to use enough `n_iter` and eyeball-check that the loss\n",
    "is converged :-)\n",
    "\n",
    "Observe how all hyper params converge. In particular, note that the constant\n",
    "mean $m(\\ve x)=c$ converges to the `const` value in `generate_data()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc059dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train mode\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "loss_func = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "n_iter = 200\n",
    "history = defaultdict(list)\n",
    "for ii in range(n_iter):\n",
    "    optimizer.zero_grad()\n",
    "    loss = -loss_func(model(X_train), y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (ii + 1) % 10 == 0:\n",
    "        print(f\"iter {ii+1}/{n_iter}, {loss=:.3f}\")\n",
    "    for p_name, p_val in extract_model_params(model).items():\n",
    "        history[p_name].append(p_val)\n",
    "    history[\"loss\"].append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9e7052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot hyper params and loss (neg. log marginal likelihood) convergence\n",
    "ncols = len(history)\n",
    "fig, axs = plt.subplots(ncols=ncols, nrows=1, figsize=(ncols * 5, 5))\n",
    "for ax, (p_name, p_lst) in zip(axs, history.items()):\n",
    "    ax.plot(p_lst)\n",
    "    ax.set_title(p_name)\n",
    "    ax.set_xlabel(\"iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1fe908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values of optimized hyper params\n",
    "pprint(extract_model_params(model, raw=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98aefb90",
   "metadata": {},
   "source": [
    "# Run prediction\n",
    "\n",
    "We show \"noiseless\" (left: $\\sigma = \\sqrt{\\mathrm{diag}(\\ma\\Sigma)}$) vs.\n",
    "\"noisy\" (right: $\\sigma = \\sqrt{\\mathrm{diag}(\\ma\\Sigma + \\sigma_n^2\\,\\ma\n",
    "I_N)}$) predictions with\n",
    "\n",
    "$$\\ma\\Sigma = \\testtest{\\ma K} - \\test{\\ma K}\\,(\\ma K+\\sigma_n^2\\,\\ma I)^{-1}\\,\\test{\\ma K}^\\top$$\n",
    "\n",
    "We find that $\\ma\\Sigma$ reflects behavior we would like to see from\n",
    "epistemic uncertainty -- it is high when we have no data\n",
    "(out-of-distribution). But this alone isn't the whole story. We need to add\n",
    "the estimated noise level $\\sigma_n^2$ in order for the confidence band to\n",
    "cover the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78de0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation (predictive posterior) mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    M = 10\n",
    "    post_pred_f = model(X_pred)\n",
    "    post_pred_y = likelihood(model(X_pred))\n",
    "\n",
    "    fig, axs = plt.subplots(ncols=2, figsize=(12, 5))\n",
    "    fig_sigmas, ax_sigmas = plt.subplots()\n",
    "    for ii, (ax, post_pred, name) in enumerate(\n",
    "        zip(axs, [post_pred_f, post_pred_y], [\"f\", \"y\"])\n",
    "    ):\n",
    "        yf_mean = post_pred.mean\n",
    "        yf_samples = post_pred.sample(sample_shape=torch.Size((M,)))\n",
    "\n",
    "        yf_std = post_pred.stddev\n",
    "        lower = yf_mean - 2 * yf_std\n",
    "        upper = yf_mean + 2 * yf_std\n",
    "        ax.plot(\n",
    "            X_train.numpy(),\n",
    "            y_train.numpy(),\n",
    "            \"o\",\n",
    "            label=\"data\",\n",
    "            color=\"tab:blue\",\n",
    "        )\n",
    "        ax.plot(\n",
    "            X_pred.numpy(),\n",
    "            yf_mean.numpy(),\n",
    "            label=\"mean\",\n",
    "            color=\"tab:red\",\n",
    "            lw=2,\n",
    "        )\n",
    "        ax.fill_between(\n",
    "            X_pred.numpy(),\n",
    "            lower.numpy(),\n",
    "            upper.numpy(),\n",
    "            label=\"confidence\",\n",
    "            color=\"tab:orange\",\n",
    "            alpha=0.3,\n",
    "        )\n",
    "        if name == \"f\":\n",
    "            sigma_label = r\"$\\pm 2\\sqrt{\\mathrm{diag}(\\Sigma)}$\"\n",
    "            zorder = 1\n",
    "        else:\n",
    "            sigma_label = (\n",
    "                r\"$\\pm 2\\sqrt{\\mathrm{diag}(\\Sigma + \\sigma_n^2\\,I)}$\"\n",
    "            )\n",
    "            zorder = 0\n",
    "        ax_sigmas.fill_between(\n",
    "            X_pred.numpy(),\n",
    "            lower.numpy(),\n",
    "            upper.numpy(),\n",
    "            label=\"confidence \" + sigma_label,\n",
    "            color=\"tab:orange\" if name == \"f\" else \"tab:blue\",\n",
    "            alpha=0.5,\n",
    "            zorder=zorder,\n",
    "        )\n",
    "        y_min = y_train.min()\n",
    "        y_max = y_train.max()\n",
    "        y_span = y_max - y_min\n",
    "        ax.set_ylim([y_min - 0.3 * y_span, y_max + 0.3 * y_span])\n",
    "        plot_samples(ax, X_pred, yf_samples, label=\"posterior pred. samples\")\n",
    "        if ii == 1:\n",
    "            ax.legend()\n",
    "    ax_sigmas.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e726d3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When running as script\n",
    "if not is_interactive():\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
